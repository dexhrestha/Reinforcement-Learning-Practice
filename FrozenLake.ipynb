{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The surface is described using a grid like the following:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                                SFFF\n",
    "                                FHFH\n",
    "                                FFFH\n",
    "                                HFFG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grid is our environment where `S` is the agent’s starting point, and it’s safe. `F` represents the frozen surface and is also safe. `H` represents a hole, and if our agent steps in a hole in the middle of a frozen lake, well, that’s not good. Finally, `G` represents the goal, which is the space on the grid where the prized frisbee is located.\n",
    "\n",
    "The agent can navigate `left`, `right`, `up`, and `down`, and the episode ends when the agent reaches the goal or falls in a hole. It receives a reward of one if it reaches the goal, and zero otherwise."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "State            Description\t                    Reward\n",
    "----------------------------------------------------------\n",
    "  S\t          Agent’s starting point - safe\t          0\n",
    "\n",
    "  F\t          Frozen surface - safe\t                  0\n",
    "\n",
    "  H\t          Hole - game over\t                      0\n",
    "\n",
    "  G\t          Goal - game over\t                      1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent has to navigate the grid by staying on the frozen surface without falling into any holes until it reaches the frisbee. If it reaches the frisbee, it wins with a reward of plus one. If it falls in a hole, it loses and receives no points for the entire episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following relation to update our Q-table values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*} q^{new}\\left( s,a\\right) =\\left( 1-\\alpha \\right) ~\\underset{\\text{old value} }{\\underbrace{q\\left( s,a\\right) }\\rule[-0.05in]{0in}{0.2in} \\rule[-0.05in]{0in}{0.2in}\\rule[-0.1in]{0in}{0.3in}}+\\alpha \\overset{\\text{ learned value}}{\\overbrace{\\left(\n",
    "                                                    R_{t+1}+\\gamma \\max_{a^{^{\\prime }}}q\\left( s^{\\prime },a^{\\prime }\\right) \\right) }} \\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "print(state_space_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1 \n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1 # Initialize exploration rate\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes = [] #'TODO: Create a list to store all rewards'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning algorithm\n",
    "\n",
    "# For each episode\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    # For each time-step within the episode\n",
    "    for step in range(max_steps_per_episode): \n",
    "        \n",
    "        # Exploration-exploitation trade-off to decide on an action\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        \n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "#             print('here')\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        # Taking the decided upon action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "#         print(new_state)\n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[state, action] = (1-learning_rate)*q_table[state,action]+learning_rate*(reward+discount_rate*np.max(q_table[new_state,:]))#'TODO: Implement Q learning update rule'\n",
    "        \n",
    "        state = new_state #'TODO: Update state with new state'                                        \n",
    "                                                 \n",
    "        # Add new reward        \n",
    "        rewards_current_episode +=  reward #'TODO: Add the new reward to total reward of episode'\n",
    "#         print(state)\n",
    "        if done == True: \n",
    "            break\n",
    "#             \"Break the loop if env state returns done, i.e. go to next episode\"                                            \n",
    "                                                                                                         \n",
    "    # Exploration rate decay\n",
    "#     print(q_table)\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)                                                 \n",
    "                                                 \n",
    "    # Add current episode reward to total rewards list\n",
    "    rewards_all_episodes.append(rewards_current_episode)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.046000000000000034\n",
      "2000 :  0.22600000000000017\n",
      "3000 :  0.43200000000000033\n",
      "4000 :  0.5390000000000004\n",
      "5000 :  0.6330000000000005\n",
      "6000 :  0.6190000000000004\n",
      "7000 :  0.6930000000000005\n",
      "8000 :  0.6830000000000005\n",
      "9000 :  0.7060000000000005\n",
      "10000 :  0.6420000000000005\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([1,2,3,5,1,231])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0.48928019 0.44784481 0.48234478 0.46183313]\n",
      " [0.29355482 0.29501351 0.29555927 0.38356953]\n",
      " [0.29766191 0.26244491 0.25960893 0.27694597]\n",
      " [0.15319479 0.07035944 0.03682483 0.06856761]\n",
      " [0.49818088 0.36306406 0.31058786 0.37275997]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.14425121 0.12756739 0.23511083 0.08106266]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.34655991 0.46704823 0.48335838 0.51978774]\n",
      " [0.40317839 0.55389818 0.47979116 0.3473756 ]\n",
      " [0.3357649  0.37076534 0.3300654  0.27428472]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.3966311  0.55330765 0.67616588 0.38466144]\n",
      " [0.740201   0.86127977 0.73522636 0.7211604 ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch our agent play Frozen Lake by playing the best action from each state according to Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "****You reached the goal!****\n"
     ]
    }
   ],
   "source": [
    "for episode in range(3):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(max_steps_per_episode):        \n",
    "        # Show current state of environment on screen\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.7)\n",
    "        \n",
    "        # Choose action with highest Q-value for current state       \n",
    "        action = np.argmax(q_table[state,:])\n",
    "        \n",
    "        # Take new action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "\n",
    "        state = new_state\n",
    "               \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
